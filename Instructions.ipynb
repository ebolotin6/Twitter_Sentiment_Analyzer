{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Program\n",
    "###### Author: Eli Bolotin\n",
    "Copyright 2018, All Rights Reserved.\n",
    "***\n",
    "\n",
    "\n",
    "## Directions\n",
    "\n",
    "**Important**: Make sure that this program's files are stored at the root level of the directory that you're running this project from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Open file StreamTweets.py\n",
    "- Define list of keywords and hashtags for each group.\n",
    "\n",
    "### Step 2. Stream tweets for group 1 and 2.\n",
    "*stream_tweets(keywords_m, option = 'user_info', file_name = 'streamed_tweets_media', max_tweets = 5000)*\n",
    "\n",
    "* Function arguments:\n",
    "    * **option**: argument must be 'user_info'\n",
    "    * **file_name**: define the name of the file to dump the tweets in JSON format. **Do not include extension**.\n",
    "    * **max_tweets**: define number of tweets to stream. \n",
    "* Function output:\n",
    "    * **file output**: json and csv files of streamed tweets.\n",
    "    * **file name**: streamed_tweets_media.json\n",
    "    \n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterProgram import MyStreamListener, stream_tweets\n",
    "\n",
    "### Step 1: define keywords and hashtags for media group\n",
    "keywords_m = ['watching show','watch season','netflix','movie','new season','watching tv','binge watching','newseries','new episode','prime video','dvr','atthemovies','film','horror','comedy','thriller','shortfilm','firstseason','secondseason','thirdseason','fourthseason','fithseason','lastseason','#watchingshow','#watchseason','#newseason','#watchingtv','#bingewatching','#newepisode','#primevideo','#edgeofmyseat','#nbc','#abc','#disney','#cnbc','#cbs','#primetime','#waitedsolong','#comedycentral']\n",
    "\n",
    "### Step 1: define keywords and hashtags for fitness group\n",
    "keywords_f = ['health fitness','fitness','legday','workoutwednesday','treadmill','pilates','yoga','gym time','deadlift','squats','FitnessFriday','gymlife','workouts','fitness training','postgym','armday','shoulderday','fitnessgoals','runner','workout','workout motivation','lift hard','lift weight','go running','crossfit','morning workout','muscle','six pack','lunges','cardio','elliptical','cycling','#health','#gymtime','#fitnesstraining','#workoutmotivation','#lifthard','#liftweight','#gorunning','#sweatforit','#morningworkout','#sixpack','#triathlon']\n",
    "\n",
    "### step 2: stream tweets for media group\n",
    "file_name_m = stream_tweets(keywords_m, option = 'user_info', file_name = 'streamed_tweets_media', max_tweets = 5000)\n",
    "\n",
    "### step 2: stream tweets for athletic group\n",
    "file_name_a = stream_tweets(keywords_f, option = 'user_info', file_name = 'streamed_tweets_fitness', max_tweets = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Open file: ProcessTweets.py. Clean the streamed tweets. Afterwards, examine the cleaned tweets.\n",
    "*group_1 = StreamedTweets(\"streamed_tweets_media.json\", sub_dir=\"Samples_Round_1\")*\n",
    "* Function arguments:\n",
    "    * **first arg**: enter **json** file_name including extension\n",
    "    * **sub_dir**: enter sub directory containing filename\n",
    "* Function output:\n",
    "    * **returns**: csv file\n",
    "    * **file output**: csv file\n",
    "    * **file name**: streamed_tweets_media_clean.csv\n",
    "\n",
    "### Step 4. Fetch other (non-filtered) tweets in full data format. Store the full data, but present the user with a column-truncated version for readability.\n",
    "*group_1.get_full_tweets(max_users = 3000)*\n",
    "* Function arguments:\n",
    "    * **max_users**: number of users to fetch from the top of the streamed tweets file\n",
    "* Function output:\n",
    "    * **returns**: json\n",
    "    * **file output**: csv and json\n",
    "    * **file name 1**: streamed_tweets_media_clean_full.json\n",
    "    * **file name 2**: streamed_tweets_media_clean_full_trunc.json\n",
    "    * **file name 3**: streamed_tweets_media_clean_full_trunc.csv\n",
    "\n",
    "###  Step 5. Generate sentiment for tweets.\n",
    "*group_1.get_sentiment()*\n",
    "* Function arguments: None\n",
    "* Function output:\n",
    "    * **returns**: csv\n",
    "    * **file output**: csv\n",
    "    * **file name**: streamed_tweets_media_clean_full_analysis.csv\n",
    "\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterProgram import StreamedTweets, TweetProgram\n",
    "\n",
    "### step 1 and 2: stream tweets (using StreamTweets.py)\n",
    "### step 3 instantiate object and clean tweets\n",
    "group_1 = StreamedTweets(\"streamed_tweets_media.json\", sub_dir=\"Samples_Round_2\")\n",
    "group_1.clean_tweets()\n",
    "\n",
    "### step 4: fetch other tweets\n",
    "group_1.get_full_tweets(max_users = 3000)\n",
    "\n",
    "### step 5 - produce sentiment analysis of cleaned tweets. \n",
    "group_1.get_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Conduct statistical analysis in R.\n",
    "\n",
    "See \"Twitter_Sentiment_Analysis.Rmd\" (or PDF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3_venv",
   "language": "python",
   "name": "py3_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
